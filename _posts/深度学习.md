



### 摘自《深度学习入门：基于Python的理论与实现》

神经网络的激活函数必须使用非线性函数。换句话说，激活函数不能使用线性函数。为什么不能使用线性函数呢？因为使用线性函数的话，加深神经网络的层数就没有意义了。

sigmoid函数：

```Python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

sigmoid函数的平滑性对神经网络的学习具有重要意义。



到目前为止，我们介绍了作为激活函数的阶跃函数和sigmoid函数。在神经网络发展的历史上，sigmoid函数很早就开始被使用了，而最近则主要使用ReLU（Rectified Linear Unit）函数。

ReLU函数：

```Python
def relu(x):
    return np.maximum(0, x)
```

本章剩余部分的内容仍将使用sigmoid函数作为激活函数，但在本书的后半部分，则将主要使用ReLU函数。(正解！)



Softmax函数，处理分类问题的：

![image-20191225183943939](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20191225183943939.png)

这样的话，输出层的各个神经元都受到所有输入信号的影响。

防止softmax溢出，这样设计函数，想法不错

```Python
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c) # 溢出对策
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```

​        输出总和为1是softmax函数的一个重要性质。正因为有了这个性质，我们才可以把softmax函数的输出解释为“概率”。

​        一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，神经网络在进行分类时，输出层的softmax函数可以省略。在实际的问题中，由于指数函数的运算需要一定的计算机运算量，因此输出层的softmax函数一般会被省略。

​         求解机器学习问题的步骤可以分为“学习” A  和“推理”两个阶段。首先，在学习阶段进行模型的学习 B ，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。如前所述，推理阶段一般会省略输出层的 softmax 函数。在输出层使用 softmax 函数是因为它和神经网络的学习有关系（详细内容请参考下一章）。

先实现神经网络的“推理处理”。这个推理处理也称为神经网络的前向传播（forward propagation）。



ReLU，计算图是真的不错的工具，哪个人才发明的。

![image-20191227171057690](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20191227171057690.png)

可以继续化简，

![image-20191227171259626](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20191227171259626.png)



​        神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”  。因此，这里将进行仿射变换的处理实现为“Affine层”。

​        几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算。

​        Affine/Softmax层，并不是新的词汇。



SGD

![image-20191228162049972](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20191228162049972.png)

表示用右边的值更新左边的值。



![image-20191228162205430](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20191228162205430.png)



**AdaGrad**

![image-20191228162624966](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20191228162624966.png)

AdaGrad会记录过去所有梯度的平方和。因此，学习越深入，更新的幅度就越小。实际上，如果无止境地学习，更新量就会变为 0，完全不再更新。为了改善这个问题，可以使用 RMSProp  [7] 方法。RMSProp方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来。这种操作从专业上讲，称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度。



Adam

看论文吧。



GoogLeNet在横向上有“宽度”，这称为“Inception结构”。



##### 数学解释：



深度学习确实需要理论上的推导：

​       从图6-18的结果可知，使用Batch Norm后，学习进行得更快了。接着，给予不同的初始值尺度，观察学习的过程如何变化。图6-19是权重初始值的标准差为各种不同的值时的学习过程图。我们发现，几乎所有的情况下都是使用Batch Norm时学习进行得更快。同时也可以发现，实际上，在不使用Batch Norm的情况下，如果不赋予一个尺度好的初始值，学习将完全无法进行。综上，通过使用Batch Norm，可以推动学习的进行。并且，对权重初始值变得健壮（“对初始值健壮”表示不那么依赖初始值）。Batch Norm具备了如此优良的性质，一定能应用在更多场合中。

………………        

**感觉很多都是根据现象猜，或者没有一点理论上的解释，很多都是直觉上的。**

**深度学习确实需要有高度抽象的数学来论证和解释。**



数据增强这种真的是属于非常垃圾的方法，数据增强又没有增加信息的量的大小，是因为NN的理论基础不够才引发的方法。

跟GAN一样，这类方法真的是丑陋异常。

还有dropout技术，也是非常垃圾。

